{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7b8c5b533cdfc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.189611Z",
     "start_time": "2024-12-10T04:28:29.180822Z"
    },
    "id": "d7b8c5b533cdfc5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import openai\n",
    "# !pip install httpx==0.27.2 --force-reinstall\n",
    "# !pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed57109e6edf7a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.199917Z",
     "start_time": "2024-12-10T04:28:29.197655Z"
    },
    "id": "ed57109e6edf7a0e"
   },
   "outputs": [],
   "source": [
    "random_state = 5814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c8e05e85b2c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.235233Z",
     "start_time": "2024-12-10T04:28:29.215195Z"
    },
    "id": "277c8e05e85b2c47"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540e791de342ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.244390Z",
     "start_time": "2024-12-10T04:28:29.242392Z"
    },
    "id": "7540e791de342ceb"
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text  = re.sub(\"#\\S*\\s\", \"\", text)\n",
    "    text  = re.sub(\"W+\", \"\", text)\n",
    "    text  = re.sub(\"@\\S*\\s\", \"\", text)\n",
    "    text  = re.sub(\"http\\S*\\s\", \"\", text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ecbb59f5918b8a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.279010Z",
     "start_time": "2024-12-10T04:28:29.251149Z"
    },
    "id": "8ecbb59f5918b8a7"
   },
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5da20674cfd177d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.288554Z",
     "start_time": "2024-12-10T04:28:29.285300Z"
    },
    "id": "5da20674cfd177d5"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['text'], data['target'],test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399591f715dc0b3",
   "metadata": {
    "id": "5399591f715dc0b3"
   },
   "source": [
    "# Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "312908c1c2c532f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.296082Z",
     "start_time": "2024-12-10T04:28:29.294448Z"
    },
    "id": "312908c1c2c532f7"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72b3b9dfbad81b3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.349041Z",
     "start_time": "2024-12-10T04:28:29.302009Z"
    },
    "id": "72b3b9dfbad81b3b"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e07fd210e7ee8e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.356580Z",
     "start_time": "2024-12-10T04:28:29.354832Z"
    },
    "id": "5e07fd210e7ee8e6"
   },
   "outputs": [],
   "source": [
    "traditional_models = {\n",
    "    'LogisticRegression' : LogisticRegression(max_iter=1000,random_state=random_state),\n",
    "    'naive_bayes' : MultinomialNB(),\n",
    "    'svm' : LinearSVC(max_iter=1000,random_state=random_state),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f17680b8a55c563e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.368630Z",
     "start_time": "2024-12-10T04:28:29.366859Z"
    },
    "id": "f17680b8a55c563e"
   },
   "outputs": [],
   "source": [
    "traditional_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54e1aec4e5b00b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.407526Z",
     "start_time": "2024-12-10T04:28:29.375111Z"
    },
    "id": "54e1aec4e5b00b4b"
   },
   "outputs": [],
   "source": [
    "for name, model in traditional_models.items():\n",
    "    model.fit(x_train_tfidf, y_train)\n",
    "    y_pred = model.predict(x_test_tfidf)\n",
    "    traditional_results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "EXvrFDERFwHJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXvrFDERFwHJ",
    "outputId": "16c712b5-1c48-401f-90f0-dc458dc9559e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LogisticRegression': {'accuracy': 0.793827971109652, 'precision': 0.7724957555178268, 'recall': 0.7165354330708661, 'f1': 0.7434640522875817}, 'naive_bayes': {'accuracy': 0.7879185817465528, 'precision': 0.782608695652174, 'recall': 0.6803149606299213, 'f1': 0.7278854254422915}, 'svm': {'accuracy': 0.7984241628365069, 'precision': 0.7671009771986971, 'recall': 0.7417322834645669, 'f1': 0.7542033626901521}}\n"
     ]
    }
   ],
   "source": [
    "print(traditional_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ebd04cc1fa399",
   "metadata": {
    "id": "b86ebd04cc1fa399"
   },
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53a882abf822b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.834875Z",
     "start_time": "2024-12-10T04:28:29.413500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d53a882abf822b6",
    "outputId": "64e291d5-8527-46fc-9e85-c01951947281"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2,hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72c96184d70365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.843628Z",
     "start_time": "2024-12-10T04:28:29.841438Z"
    },
    "id": "ec72c96184d70365"
   },
   "outputs": [],
   "source": [
    "def prep_data_bert(texts, labels):\n",
    "    texts = texts.astype('str').tolist()\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, max_length = 128,batch_size =12,return_tensors='pt')\n",
    "    if labels is not None:\n",
    "        dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels.values))\n",
    "    else:\n",
    "        dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "\n",
    "    return DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54fc7e17558c2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.851977Z",
     "start_time": "2024-12-10T04:28:29.849813Z"
    },
    "id": "9f54fc7e17558c2f"
   },
   "outputs": [],
   "source": [
    "def train_bert(train_loader, epochs = 3):\n",
    "    optimizer = torch.optim.AdamW(bert_model.parameters(), lr=2e-5, weight_decay = 0.01)\n",
    "    early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        bert_model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(bert_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e3fad38b0b0f321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:29.860051Z",
     "start_time": "2024-12-10T04:28:29.858105Z"
    },
    "id": "9e3fad38b0b0f321"
   },
   "outputs": [],
   "source": [
    "def bert_predict(test_loader):\n",
    "    bert_model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "            predictions.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2aa156749bda4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:28:30.035870Z",
     "start_time": "2024-12-10T04:28:29.866185Z"
    },
    "id": "ce2aa156749bda4b"
   },
   "outputs": [],
   "source": [
    "train_loader = prep_data_bert(x_train, y_train)\n",
    "test_loader = prep_data_bert(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acf86fab4e93f223",
   "metadata": {
    "id": "acf86fab4e93f223"
   },
   "outputs": [],
   "source": [
    "bert_predictions = bert_predict(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac2d129ff5673b",
   "metadata": {
    "id": "b8ac2d129ff5673b"
   },
   "source": [
    "# GPT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cbe3f15045978",
   "metadata": {
    "id": "214cbe3f15045978"
   },
   "outputs": [],
   "source": [
    "sample_size = 4\n",
    "api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0vda3h_frb3g",
   "metadata": {
    "id": "0vda3h_frb3g"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "def gpt_predictions(texts, api_key, few_shot_samples=16):\n",
    "    openai.api_key = api_key\n",
    "    predictions = []\n",
    "\n",
    "    template = \"\"\"You are a highly trained assistant tasked with\n",
    "            classifying tweets to determine if they are about a disaster or not.\n",
    "\n",
    "        Your job is to read each tweet carefully and provide a classification as either:\n",
    "        - 1 if the tweet is about a disaster\n",
    "        - 0 if the tweet is not about a disaster\n",
    "\n",
    "        For each classification,\n",
    "        output the classification as a number (either 0 or 1),\n",
    "        without any additional text or explanation.\n",
    "\n",
    "        Here are some examples to help you understand the task:\n",
    "        Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.',\n",
    "        Classification: 1\n",
    "        Tweet: 'I love spending time with my friends at the beach.',\n",
    "        Classification: 0\n",
    "        Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.',\n",
    "        Classification: 1\n",
    "        Tweet: 'It’s a beautiful day for a walk in the park!',\n",
    "        Classification: 0\n",
    "\n",
    "        Now, please classify the following tweet:\n",
    "        Tweet: {tweet}, Classification:\n",
    "    \"\"\"\n",
    "\n",
    "    for text in texts:\n",
    "        formatted_prompt = template.format(tweet=text)\n",
    "        print(f\"Formatted Prompt for Classification:\\n{formatted_prompt}\")\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies tweets as related to disasters or not.\"},\n",
    "                {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            ],\n",
    "            max_tokens=10,\n",
    "            temperature=0.5\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            prediction_text = response.choices[0].message.content.strip()\n",
    "            print(f\"Prediction for tweet '{text}': {prediction_text}\")\n",
    "            if prediction_text.isdigit():\n",
    "                prediction = int(prediction_text)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid response format\")\n",
    "            predictions.append(prediction)\n",
    "        except (ValueError, KeyError) as e:\n",
    "            print(f\"Error processing response for '{text}': {e}\")\n",
    "            predictions.append(0)\n",
    "\n",
    "        # to avoid hitting rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "McoLBc2cDs02",
   "metadata": {
    "id": "McoLBc2cDs02"
   },
   "outputs": [],
   "source": [
    "sample_ids = np.random.choice(len(x_test), sample_size, replace=False)\n",
    "x_test_samples = x_test.iloc[sample_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "rVOECxT1s0f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVOECxT1s0f0",
    "outputId": "f1124cf0-373f-440d-9cef-1c9a0032dc18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7442    rt dianneg gunshot wound 9 is in the bicep onl...\n",
      "4417    ûgood samaritansûª shot in horror hijacking ht...\n",
      "3117    i was blow drying my hair amp the cable caught...\n",
      "1750    motorcyclist bicyclist injured in denver colli...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "nlPboIJmsyEQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlPboIJmsyEQ",
    "outputId": "6a35199a-c23e-49c9-8ba1-76cae8948048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt for Classification:\n",
      "You are a highly trained assistant tasked with classifying tweets to determine if they are about a disaster or not.\n",
      "        Your job is to read each tweet carefully and provide a classification as either:\n",
      "        - 1 if the tweet is about a disaster\n",
      "        - 0 if the tweet is not about a disaster\n",
      "    For each classification, output the classification as a number (either 0 or 1), without any additional text or explanation.\n",
      "\n",
      "    Here are some examples to help you understand the task:\n",
      "    Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.', Classification: 1\n",
      "    Tweet: 'I love spending time with my friends at the beach.', Classification: 0\n",
      "    Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.', Classification: 1\n",
      "    Tweet: 'It’s a beautiful day for a walk in the park!', Classification: 0\n",
      "\n",
      "    Now, please classify the following tweet:\n",
      "    Tweet: rt dianneg gunshot wound 9 is in the bicep only 1 of the 10 wounds that is not in the chesttorso area kerricktrial jonathanferrell, Classification:\n",
      "    \n",
      "Prediction for tweet 'rt dianneg gunshot wound 9 is in the bicep only 1 of the 10 wounds that is not in the chesttorso area kerricktrial jonathanferrell': 1\n",
      "Formatted Prompt for Classification:\n",
      "You are a highly trained assistant tasked with classifying tweets to determine if they are about a disaster or not.\n",
      "        Your job is to read each tweet carefully and provide a classification as either:\n",
      "        - 1 if the tweet is about a disaster\n",
      "        - 0 if the tweet is not about a disaster\n",
      "    For each classification, output the classification as a number (either 0 or 1), without any additional text or explanation.\n",
      "\n",
      "    Here are some examples to help you understand the task:\n",
      "    Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.', Classification: 1\n",
      "    Tweet: 'I love spending time with my friends at the beach.', Classification: 0\n",
      "    Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.', Classification: 1\n",
      "    Tweet: 'It’s a beautiful day for a walk in the park!', Classification: 0\n",
      "\n",
      "    Now, please classify the following tweet:\n",
      "    Tweet: ûgood samaritansûª shot in horror hijacking httptcov5yuualoqw 263chat twimbos zimpapersviews, Classification:\n",
      "    \n",
      "Prediction for tweet 'ûgood samaritansûª shot in horror hijacking httptcov5yuualoqw 263chat twimbos zimpapersviews': 1\n",
      "Formatted Prompt for Classification:\n",
      "You are a highly trained assistant tasked with classifying tweets to determine if they are about a disaster or not.\n",
      "        Your job is to read each tweet carefully and provide a classification as either:\n",
      "        - 1 if the tweet is about a disaster\n",
      "        - 0 if the tweet is not about a disaster\n",
      "    For each classification, output the classification as a number (either 0 or 1), without any additional text or explanation.\n",
      "\n",
      "    Here are some examples to help you understand the task:\n",
      "    Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.', Classification: 1\n",
      "    Tweet: 'I love spending time with my friends at the beach.', Classification: 0\n",
      "    Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.', Classification: 1\n",
      "    Tweet: 'It’s a beautiful day for a walk in the park!', Classification: 0\n",
      "\n",
      "    Now, please classify the following tweet:\n",
      "    Tweet: i was blow drying my hair amp the cable caught on fire i let go of it as soon as i realized just before i could get electrocuted, Classification:\n",
      "    \n",
      "Prediction for tweet 'i was blow drying my hair amp the cable caught on fire i let go of it as soon as i realized just before i could get electrocuted': 1\n",
      "Formatted Prompt for Classification:\n",
      "You are a highly trained assistant tasked with classifying tweets to determine if they are about a disaster or not.\n",
      "        Your job is to read each tweet carefully and provide a classification as either:\n",
      "        - 1 if the tweet is about a disaster\n",
      "        - 0 if the tweet is not about a disaster\n",
      "    For each classification, output the classification as a number (either 0 or 1), without any additional text or explanation.\n",
      "\n",
      "    Here are some examples to help you understand the task:\n",
      "    Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.', Classification: 1\n",
      "    Tweet: 'I love spending time with my friends at the beach.', Classification: 0\n",
      "    Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.', Classification: 1\n",
      "    Tweet: 'It’s a beautiful day for a walk in the park!', Classification: 0\n",
      "\n",
      "    Now, please classify the following tweet:\n",
      "    Tweet: motorcyclist bicyclist injured in denver collision on broadway at least two people were taken to a localû_ httptcoozk1qhjvfh, Classification:\n",
      "    \n",
      "Prediction for tweet 'motorcyclist bicyclist injured in denver collision on broadway at least two people were taken to a localû_ httptcoozk1qhjvfh': 1\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = gpt_predictions(x_test_samples, api_key=api_key)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8e765a6b09fb3",
   "metadata": {
    "id": "e7b8e765a6b09fb3"
   },
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b33150c47a1c970",
   "metadata": {
    "id": "9b33150c47a1c970"
   },
   "outputs": [],
   "source": [
    "def eval_models(y_true, y_pred, model):\n",
    "    return {\n",
    "        'Model' : model,\n",
    "        'Precision' : precision_score(y_true, y_pred),\n",
    "        'Recall' : recall_score(y_true, y_pred),\n",
    "        'F1' : f1_score(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e59b4155733d0bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e59b4155733d0bc",
    "outputId": "3f9a3eb5-9674-4147-de03-b1aa21c26e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Model': 'LogisticRegression', 'accuracy': 0.7984241628365069, 'precision': 0.7671009771986971, 'recall': 0.7417322834645669, 'f1': 0.7542033626901521}, {'Model': 'naive_bayes', 'accuracy': 0.7984241628365069, 'precision': 0.7671009771986971, 'recall': 0.7417322834645669, 'f1': 0.7542033626901521}, {'Model': 'svm', 'accuracy': 0.7984241628365069, 'precision': 0.7671009771986971, 'recall': 0.7417322834645669, 'f1': 0.7542033626901521}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, metrics in traditional_models.items():\n",
    "    y_pred = model.predict(x_test_tfidf)\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred)\n",
    "    })\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l4GzZLhUF6C7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4GzZLhUF6C7",
    "outputId": "75cb996e-9d36-4295-ff2a-fbda1a76159c"
   },
   "outputs": [],
   "source": [
    "print(eval_models(y_test, bert_predictions[:len(y_test)], 'BERT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "KdR1D6nNufAh",
   "metadata": {
    "id": "KdR1D6nNufAh"
   },
   "outputs": [],
   "source": [
    "y_test_samples = y_test.iloc[sample_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "EDkzGwrJF-hq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDkzGwrJF-hq",
    "outputId": "467c0d65-f9cd-492b-a55d-d929009f23eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt for Classification:\n",
      "You are a highly trained assistant tasked with classifying tweets to determine if they are about a disaster or not.\n",
      "        Your job is to read each tweet carefully and provide a classification as either:\n",
      "        - 1 if the tweet is about a disaster\n",
      "        - 0 if the tweet is not about a disaster\n",
      "    For each classification, output the classification as a number (either 0 or 1), without any additional text or explanation.\n",
      "\n",
      "    Here are some examples to help you understand the task:\n",
      "    Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.', Classification: 1\n",
      "    Tweet: 'I love spending time with my friends at the beach.', Classification: 0\n",
      "    Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.', Classification: 1\n",
      "    Tweet: 'It’s a beautiful day for a walk in the park!', Classification: 0\n",
      "\n",
      "    Now, please classify the following tweet:\n",
      "    Tweet: rt dianneg gunshot wound 9 is in the bicep only 1 of the 10 wounds that is not in the chesttorso area kerricktrial jonathanferrell, Classification:\n",
      "    \n",
      "Prediction for tweet 'rt dianneg gunshot wound 9 is in the bicep only 1 of the 10 wounds that is not in the chesttorso area kerricktrial jonathanferrell': 1\n",
      "Formatted Prompt for Classification:\n",
      "You are a highly trained assistant tasked with classifying tweets to determine if they are about a disaster or not.\n",
      "        Your job is to read each tweet carefully and provide a classification as either:\n",
      "        - 1 if the tweet is about a disaster\n",
      "        - 0 if the tweet is not about a disaster\n",
      "    For each classification, output the classification as a number (either 0 or 1), without any additional text or explanation.\n",
      "\n",
      "    Here are some examples to help you understand the task:\n",
      "    Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.', Classification: 1\n",
      "    Tweet: 'I love spending time with my friends at the beach.', Classification: 0\n",
      "    Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.', Classification: 1\n",
      "    Tweet: 'It’s a beautiful day for a walk in the park!', Classification: 0\n",
      "\n",
      "    Now, please classify the following tweet:\n",
      "    Tweet: ûgood samaritansûª shot in horror hijacking httptcov5yuualoqw 263chat twimbos zimpapersviews, Classification:\n",
      "    \n",
      "Prediction for tweet 'ûgood samaritansûª shot in horror hijacking httptcov5yuualoqw 263chat twimbos zimpapersviews': 1\n",
      "Formatted Prompt for Classification:\n",
      "You are a highly trained assistant tasked with classifying tweets to determine if they are about a disaster or not.\n",
      "        Your job is to read each tweet carefully and provide a classification as either:\n",
      "        - 1 if the tweet is about a disaster\n",
      "        - 0 if the tweet is not about a disaster\n",
      "    For each classification, output the classification as a number (either 0 or 1), without any additional text or explanation.\n",
      "\n",
      "    Here are some examples to help you understand the task:\n",
      "    Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.', Classification: 1\n",
      "    Tweet: 'I love spending time with my friends at the beach.', Classification: 0\n",
      "    Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.', Classification: 1\n",
      "    Tweet: 'It’s a beautiful day for a walk in the park!', Classification: 0\n",
      "\n",
      "    Now, please classify the following tweet:\n",
      "    Tweet: i was blow drying my hair amp the cable caught on fire i let go of it as soon as i realized just before i could get electrocuted, Classification:\n",
      "    \n",
      "Prediction for tweet 'i was blow drying my hair amp the cable caught on fire i let go of it as soon as i realized just before i could get electrocuted': 1\n",
      "Formatted Prompt for Classification:\n",
      "You are a highly trained assistant tasked with classifying tweets to determine if they are about a disaster or not.\n",
      "        Your job is to read each tweet carefully and provide a classification as either:\n",
      "        - 1 if the tweet is about a disaster\n",
      "        - 0 if the tweet is not about a disaster\n",
      "    For each classification, output the classification as a number (either 0 or 1), without any additional text or explanation.\n",
      "\n",
      "    Here are some examples to help you understand the task:\n",
      "    Tweet: 'The storm has caused severe flooding in the city, with many homes submerged.', Classification: 1\n",
      "    Tweet: 'I love spending time with my friends at the beach.', Classification: 0\n",
      "    Tweet: 'Earthquake tremors were felt throughout the region, with significant damage reported.', Classification: 1\n",
      "    Tweet: 'It’s a beautiful day for a walk in the park!', Classification: 0\n",
      "\n",
      "    Now, please classify the following tweet:\n",
      "    Tweet: motorcyclist bicyclist injured in denver collision on broadway at least two people were taken to a localû_ httptcoozk1qhjvfh, Classification:\n",
      "    \n",
      "Prediction for tweet 'motorcyclist bicyclist injured in denver collision on broadway at least two people were taken to a localû_ httptcoozk1qhjvfh': 1\n",
      "{'Model': 'GPT', 'Precision': 0.75, 'Recall': 1.0, 'F1': 0.8571428571428571}\n"
     ]
    }
   ],
   "source": [
    "print(eval_models(y_test_samples, gpt_predictions(x_test_samples, api_key = api_key), 'GPT'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
